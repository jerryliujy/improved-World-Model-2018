{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **World Model - Memory *(M)*** \n",
    "\n",
    "This notebook implements the Memory component of a World Model framework. The Memory module uses a combination of recurrent neural networks and mixture density networks to predict future states based on current observations and actions.\n",
    "\n",
    "### **Workflow Overview**\n",
    "- **Data Generation and Processing:** Collect state-action sequences from environment interactions\n",
    "- **Latent Vector Extraction:** Convert raw observations into compact latent representations (z)\n",
    "- **Dataset Creation:** Prepare sequential data for MDN-RNN training\n",
    "- **MDN-RNN Model Implementation:** Build and train the predictive memory model\n",
    "- **Visualization and Evaluation:** Analyze model performance through reconstructions\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Data Preparation: CarRacingDataset**\n",
    "---\n",
    "The `CarRacingDataset` class creates a PyTorch dataset from pre-recorded environment interactions stored in HDF5 format.\n",
    "\n",
    "#### **Key Features**\n",
    "\n",
    "- **Image Transformation:** Uses a pre-trained Variational Autoencoder (VAE) to encode raw images into latent vectors (z)\n",
    "- **Sequence Generation:** Prepares temporal sequences for LSTM training\n",
    "- **Data Structure:**\n",
    "  - **z_t**: Current state in latent space\n",
    "  - **z_{t+1}**: Next state (prediction target)\n",
    "  - **action_t**: Action taken at time t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "class CarRacingDataset(Dataset):\n",
    "    def __init__(self, h5_path='car_racing_data.h5', transform=None, vae=None, device=None):\n",
    "        self.h5_path = h5_path\n",
    "        self.transform = transform if transform is not None else transforms.ToTensor()\n",
    "        self.vae = vae\n",
    "        self.device = device\n",
    "\n",
    "        # Open HDF5 to get the dimensions of the dataset\n",
    "        with h5py.File(self.h5_path, 'r') as h5f:\n",
    "            self.num_episodes = h5f['images'].shape[0]\n",
    "            self.max_steps = h5f['images'].shape[1]\n",
    "\n",
    "        self.h5_file = None \n",
    "        self.vae.eval() \n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_episodes\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieves the episode corresponding to the given index.\n",
    "        \n",
    "        Args:\n",
    "            idx (int): index of the episode to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary containing:\n",
    "                - 'z': Shape tensor (episode_length, z_dim).\n",
    "                - 'z_next': Shape tensor (episode_length, z_dim)\n",
    "                - 'action': Shape tensor (episode_length, action_dim)\n",
    "                - 'reward': Shape tensor (episode_length,)\n",
    "                - 'done': Shape tensor (episode_length,)\n",
    "        \"\"\"\n",
    "        if self.h5_file is None:\n",
    "            self.h5_file = h5py.File(self.h5_path, 'r')\n",
    "\n",
    "        images = self.h5_file['images'][idx]       \n",
    "        actions = self.h5_file['actions'][idx]     \n",
    "        rewards = self.h5_file['rewards'][idx]     \n",
    "        dones = self.h5_file['dones'][idx]        \n",
    "\n",
    "        # Process each sequence to get the encoded images\n",
    "        with torch.no_grad():\n",
    "            img_tensors = torch.stack([self.transform(img) for img in images])  \n",
    "            img_tensors = img_tensors.to(self.device)\n",
    "            mu, logvar = self.vae.encode(img_tensors)\n",
    "            z = self.vae.reparameterize(mu, logvar)  \n",
    "            \n",
    "        # Obtain the next images z_(t +1)\n",
    "        z_next = z[1:]  \n",
    "        z = z[:-1]      \n",
    "\n",
    "        actions = torch.tensor(actions[:-1], dtype=torch.float32)\n",
    "        rewards = torch.tensor(rewards[:-1], dtype=torch.float32)\n",
    "        dones =  torch.tensor(dones[:-1], dtype=torch.float32)\n",
    "\n",
    "\n",
    "        return z, z_next, actions, rewards, dones\n",
    "\n",
    "\n",
    "    def __del__(self):\n",
    "        if self.h5_file is not None:\n",
    "            self.h5_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. RNN-MDN Model Architecture**\n",
    "---\n",
    "\n",
    "The MDN-RNN model combines two components:\n",
    "1. A **Long Short-Term Memory (LSTM)** network that captures temporal dependencies\n",
    "2. A **Mixture Density Network (MDN)** that outputs probability distributions rather than point estimates\n",
    "\n",
    "### Model Function\n",
    "\n",
    "The MDN-RNN predicts the distribution of the next latent state given the current state, action, and hidden state:\n",
    "\n",
    "$$P(z_{t+1} \\mid a_t, z_t, h_t)$$\n",
    "\n",
    "<img src=\"imgs/rnn_mdn.png\" width=\"600\">\n",
    "\n",
    "### Component Details\n",
    "\n",
    "#### LSTM (Long Short-Term Memory)\n",
    "- Functions as the memory component, maintaining information about past states and actions\n",
    "- Processes sequences of latent vectors and actions\n",
    "- Updates its hidden state to capture temporal dependencies\n",
    "\n",
    "#### MDN (Mixture Density Network)\n",
    "- Predicts a mixture of Gaussian distributions rather than a single output value\n",
    "- Enables the model to capture uncertainty and multimodal predictions\n",
    "- Outputs three parameter sets:\n",
    "  - **π (pi)**: Mixture weights (probabilities summing to 1)\n",
    "  - **μ (mu)**: Means of Gaussian components\n",
    "  - **σ (sigma)**: Standard deviations of Gaussian components\n",
    "\n",
    "<img src=\"imgs/rnn_mdn_2.png\" alt=\"picture mdn\" width=\"800\"/>\n",
    "\n",
    "### Loss Function: Negative Log-Likelihood (NLL)\n",
    "\n",
    "For a mixture of Gaussians, the probability of a value x is:\n",
    "\n",
    "$$P(x|\\mu_k, \\sigma_k) = \\frac{1}{\\sqrt{2 \\pi \\sigma_k^2}} \\exp\\left( -\\frac{(x - \\mu_k)^2}{2 \\sigma_k^2} \\right)$$\n",
    "\n",
    "The negative log-likelihood loss is:\n",
    "\n",
    "$$\\text{Loss} = -\\log \\left( \\sum_{k=1}^{M} \\pi_k \\cdot \\text{P}(x|\\mu_k, \\sigma_k) \\right)$$\n",
    "\n",
    "### Numerical Stability: Log-Sum-Exp Trick\n",
    "\n",
    "To prevent underflow issues when computing probabilities of the mixture components, we use the log-sum-exp trick:\n",
    "\n",
    "$$\\mathcal{L} = -\\log \\left( \\sum_{k=1}^{M} \\pi_k \\cdot \\text{P}(x|\\mu_k, \\sigma_k) \\right) = -\\log \\left( \\sum_{k=1}^{M} \\exp \\left( \\log (\\pi_k) + \\log (P(x \\mid \\mu_k, \\sigma_k)) \\right) \\right)$$\n",
    "\n",
    "With the trick applied:\n",
    "\n",
    "$$\\mathcal{L} = - \\left( \\max_k \\log(a_k) + \\log \\left( \\sum_{k=1}^{M} \\exp(\\log(a_k) - \\max_k \\log(a_k)) \\right) \\right)$$\n",
    "\n",
    "where $a_k = \\pi_k \\cdot \\text{P}(x|\\mu_k, \\sigma_k)$\n",
    "\n",
    "PyTorch provides an optimized implementation via `torch.logsumexp()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Recurrent neural network that processes latent states and actions.\n",
    "    \n",
    "    Args:\n",
    "        latent_dim: Dimension of the latent state vector\n",
    "        action_dim: Dimension of the action vector\n",
    "        hidden_dim: Dimension of the hidden state\n",
    "        num_layers: Number of LSTM layers\n",
    "    \"\"\"\n",
    "    def __init__(self, latent_dim, action_dim, hidden_dim, num_layers=1):\n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=latent_dim + action_dim, \n",
    "                            hidden_size=hidden_dim, \n",
    "                            num_layers=num_layers, \n",
    "                            batch_first=True)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "    def forward(self, z, a, h=None):\n",
    "        \"\"\"\n",
    "        Forward pass through the RNN.\n",
    "        \n",
    "        Args:\n",
    "            z: Latent state tensor [batch_size, seq_len, latent_dim]\n",
    "            a: Action tensor [batch_size, seq_len, action_dim]\n",
    "            h: Hidden state tuple (h_n, c_n) or None for zero initialization\n",
    "            \n",
    "        Returns:\n",
    "            outs_rnn: Output tensor [batch_size, seq_len, hidden_dim]\n",
    "            h: Updated hidden state tuple (h_n, c_n)\n",
    "        \"\"\"\n",
    "        x = torch.cat([z, a], dim=-1)\n",
    "        outs_rnn, h = self.lstm(x, h)\n",
    "        return outs_rnn, h\n",
    "    \n",
    "    def init_hidden(self, batch_size, device='cpu'):\n",
    "        \"\"\"Initialize hidden state with zeros\"\"\"\n",
    "        return (torch.zeros(1, batch_size, self.hidden_dim).to(device),\n",
    "                torch.zeros(1, batch_size, self.hidden_dim).to(device))\n",
    "    \n",
    "    \n",
    "class MDN(nn.Module):\n",
    "    \"\"\"\n",
    "    Mixture Density Network that produces parameters for a mixture of Gaussians.\n",
    "    \n",
    "    Args:\n",
    "        latent_dim: Dimension of the latent state vector\n",
    "        hidden_dim: Dimension of the hidden state from RNN\n",
    "        num_gaussians: Number of Gaussian components in the mixture\n",
    "    \"\"\"\n",
    "    def __init__(self, latent_dim, hidden_dim, num_gaussians):\n",
    "        super(MDN, self).__init__()\n",
    "        self.num_gaussians = num_gaussians\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        # Network layers for mixture parameters\n",
    "        self.fc_pi = nn.Linear(hidden_dim, num_gaussians)          # Mixture weights (π)\n",
    "        self.fc_mu = nn.Linear(hidden_dim, num_gaussians * latent_dim)   # Means (μ)\n",
    "        self.fc_sigma = nn.Linear(hidden_dim, num_gaussians * latent_dim) # Std devs (σ)\n",
    "        \n",
    "    def forward(self, outs_rnn, tau):\n",
    "        \"\"\"\n",
    "        Forward pass through the MDN.\n",
    "        \n",
    "        Args:\n",
    "            outs_rnn: RNN output tensor [batch_size, seq_len, hidden_dim]\n",
    "            tau: Temperature parameter for controlling prediction sharpness\n",
    "            \n",
    "        Returns:\n",
    "            pi: Mixture weights [batch_size, seq_len, num_gaussians]\n",
    "            mu: Gaussian means [batch_size, seq_len, num_gaussians, latent_dim]\n",
    "            sigma: Gaussian std devs [batch_size, seq_len, num_gaussians, latent_dim]\n",
    "        \"\"\"\n",
    "        tau = torch.tensor(tau).to(outs_rnn.device)\n",
    "        \n",
    "        pi = self.fc_pi(outs_rnn)\n",
    "        mu = self.fc_mu(outs_rnn)\n",
    "        sigma = self.fc_sigma(outs_rnn)\n",
    "        \n",
    "        # Reshape outputs to [batch_size, seq_len, num_gaussians, latent_dim]\n",
    "        batch_size, seq_length, _ = outs_rnn.size()\n",
    "        mu = mu.view(batch_size, seq_length, self.num_gaussians, self.latent_dim)\n",
    "        sigma = sigma.view(batch_size, seq_length, self.num_gaussians, self.latent_dim)\n",
    "        \n",
    "        # Ensure sigma is positive and apply temperature scaling\n",
    "        sigma = torch.exp(sigma) + 1e-15\n",
    "        sigma = sigma * torch.sqrt(tau)  # Scale sigma with temperature\n",
    "        \n",
    "        # Apply temperature to mixture weights and ensure they sum to 1\n",
    "        pi = F.softmax(pi/tau, dim=-1) + 1e-15\n",
    "        \n",
    "        return pi, mu, sigma\n",
    "        \n",
    "\n",
    "class MDNRNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Combined MDN-RNN model that predicts distributions over future latent states.\n",
    "    \n",
    "    Args:\n",
    "        latent_dim: Dimension of the latent state vector\n",
    "        action_dim: Dimension of the action vector\n",
    "        hidden_dim: Dimension of the RNN hidden state\n",
    "        num_gaussians: Number of Gaussian components in the mixture\n",
    "        num_layers: Number of LSTM layers\n",
    "    \"\"\"\n",
    "    def __init__(self, latent_dim, action_dim, hidden_dim, num_gaussians, num_layers=1):\n",
    "        super(MDNRNN, self).__init__()\n",
    "        \n",
    "        self.rnn = RNN(latent_dim, action_dim, hidden_dim, num_layers)\n",
    "        self.mdn = MDN(latent_dim, hidden_dim, num_gaussians)\n",
    "        \n",
    "    def forward(self, z, a, tau=1.0, h=None):\n",
    "        \"\"\"\n",
    "        Forward pass through the combined MDN-RNN model.\n",
    "        \n",
    "        Args:\n",
    "            z: Latent state tensor [batch_size, seq_len, latent_dim]\n",
    "            a: Action tensor [batch_size, seq_len, action_dim]\n",
    "            tau: Temperature parameter for controlling prediction sharpness\n",
    "            h: Hidden state tuple (h_n, c_n) or None\n",
    "            \n",
    "        Returns:\n",
    "            pi: Mixture weights [batch_size, seq_len, num_gaussians]\n",
    "            mu: Gaussian means [batch_size, seq_len, num_gaussians, latent_dim]\n",
    "            sigma: Gaussian std devs [batch_size, seq_len, num_gaussians, latent_dim]\n",
    "            h: Updated hidden state tuple (h_n, c_n)\n",
    "        \"\"\"\n",
    "        outs_rnn, h = self.rnn(z, a, h)\n",
    "        pi, mu, sigma = self.mdn(outs_rnn, tau)\n",
    "        \n",
    "        return pi, mu, sigma, h\n",
    "\n",
    "\n",
    "def sample_mdn(pi, mu, sigma):\n",
    "    \"\"\"\n",
    "    Sample a latent vector from the mixture distribution.\n",
    "    \n",
    "    Args:\n",
    "        pi: Mixture weights [1, 1, num_gaussians]\n",
    "        mu: Gaussian means [1, 1, num_gaussians, latent_dim]\n",
    "        sigma: Gaussian std devs [1, 1, num_gaussians, latent_dim]\n",
    "        \n",
    "    Returns:\n",
    "        z: Sampled latent vector [1, 1, latent_dim]\n",
    "    \"\"\"\n",
    "    # Select a Gaussian component based on mixture weights\n",
    "    component_index = torch.multinomial(pi.view(-1), 1).item()  \n",
    "\n",
    "    # Get the mean and std dev for the selected component\n",
    "    selected_mu = mu[0, 0, component_index]   \n",
    "    selected_sigma = sigma[0, 0, component_index]  \n",
    "\n",
    "    # Sample from the Gaussian distribution\n",
    "    epsilon = torch.randn(selected_mu.size(), device=mu.device)\n",
    "    z = selected_mu + selected_sigma * epsilon\n",
    "\n",
    "    return z.unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "\n",
    "def gaussian_nll_loss(pi, mu, sigma, z_next):\n",
    "    \"\"\"\n",
    "    Compute the negative log-likelihood loss for a mixture of Gaussians.\n",
    "    \n",
    "    Args:\n",
    "        pi: Mixture weights [batch, seq, num_gaussians]\n",
    "        mu: Gaussian means [batch, seq, num_gaussians, latent_dim]\n",
    "        sigma: Gaussian std devs [batch, seq, num_gaussians, latent_dim]\n",
    "        z_next: Target latent vectors [batch, seq, latent_dim]\n",
    "        \n",
    "    Returns:\n",
    "        nll: Scalar negative log-likelihood loss\n",
    "    \"\"\"\n",
    "    z_next = z_next.unsqueeze(2)  # Add dim for num_gaussians: [batch, seq, 1, latent_dim]\n",
    "    log_pi = torch.log(pi)\n",
    "    \n",
    "    # Create normal distributions for each mixture component\n",
    "    normal_dist = Normal(loc=mu, scale=sigma)  # [batch, seq, num_gaussians, latent_dim]\n",
    "\n",
    "    # Compute log probabilities for the target z_next\n",
    "    log_prob = normal_dist.log_prob(z_next)  # [batch, seq, num_gaussians, latent_dim]\n",
    "\n",
    "    # Sum log probabilities across the latent dimension\n",
    "    log_prob = log_prob.sum(-1)  # [batch, seq, num_gaussians]\n",
    "\n",
    "    # Use log-sum-exp trick for numerical stability\n",
    "    log_sum_exp = torch.logsumexp(log_pi + log_prob, dim=-1)  # [batch, seq]\n",
    "\n",
    "    # Mean negative log-likelihood\n",
    "    nll = -log_sum_exp.mean()\n",
    "    return nll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. Model Training and Evaluation**\n",
    "---\n",
    "\n",
    "### Model Instantiation and Summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MDNRNN(\n",
      "  (rnn): RNN(\n",
      "    (lstm): LSTM(35, 256, batch_first=True)\n",
      "  )\n",
      "  (mdn): MDN(\n",
      "    (fc_pi): Linear(in_features=256, out_features=5, bias=True)\n",
      "    (fc_mu): Linear(in_features=256, out_features=160, bias=True)\n",
      "    (fc_sigma): Linear(in_features=256, out_features=160, bias=True)\n",
      "  )\n",
      ")\n",
      "Number of parameters: 383,557\n"
     ]
    }
   ],
   "source": [
    "mdnrnn = MDNRNN(latent_dim=32, action_dim=3, hidden_dim=256, num_gaussians=5)\n",
    "\n",
    "print(mdnrnn)\n",
    "total_params = sum(p.numel() for p in mdnrnn.parameters())\n",
    "print(f\"Number of parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Function\n",
    "\n",
    "This function trains the MDN-RNN model with gradient clipping for stability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm \n",
    "import os \n",
    "\n",
    "def train_model(mdnrnn, dataloader, optimizer, num_epochs, device, save=True, tau=1.0, name='memory', path='checkpoints'):\n",
    "    \"\"\"\n",
    "    Train the MDN-RNN model.\n",
    "    \n",
    "    Args:\n",
    "        mdnrnn: The MDN-RNN model instance\n",
    "        dataloader: DataLoader containing training data\n",
    "        optimizer: Optimizer for parameter updates\n",
    "        num_epochs: Number of training epochs\n",
    "        device: Device to train on (cpu/cuda)\n",
    "        save: Whether to save the model\n",
    "        tau: Temperature parameter for MDN\n",
    "        name: Name for the saved model file\n",
    "    \n",
    "    Returns:\n",
    "        loss_history: List of average losses per epoch\n",
    "    \"\"\"\n",
    "    mdnrnn.train()\n",
    "    loss_history = []\n",
    "\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "    # Apply gradient clipping to prevent exploding gradients\n",
    "    torch.nn.utils.clip_grad_norm_(mdnrnn.parameters(), max_norm=1.0)\n",
    "        \n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "        \n",
    "        with tqdm(total=len(dataloader), desc=f\"Epoch ({epoch+1}/{num_epochs})\", unit=\"batch\") as pbar:\n",
    "            \n",
    "            for z, z_next, actions, rewards, dones in dataloader:\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                z, z_next, a = z.to(device), z_next.to(device), actions.to(device)\n",
    "\n",
    "                pi, mu, sigma, h = mdnrnn(z, a, tau=tau)\n",
    "                \n",
    "                # Calculate loss and track reconstruction quality                            \n",
    "                loss = gaussian_nll_loss(pi, mu, sigma, z_next)\n",
    "                recon_loss = torch.abs(z_next.unsqueeze(2) - mu).mean().item()\n",
    "\n",
    "                # Backpropagation\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "                \n",
    "                # Update progress bar\n",
    "                pbar.update(1)\n",
    "                pbar.set_postfix({'NLL Loss': f'{loss.item():.4f} | Reconstruction Loss: {recon_loss:.4f}'})\n",
    "    \n",
    "        # Track epoch loss\n",
    "        average_loss = total_loss / len(dataloader)\n",
    "        loss_history.append(average_loss)\n",
    "        \n",
    "        pbar.set_postfix({'NLL Loss': f'{average_loss:.4f} | Reconstruction Loss: {recon_loss:.4f}'})\n",
    "    \n",
    "    if save:\n",
    "        torch.save(mdnrnn.state_dict(), f'{path}/{name}.pth')\n",
    "        print(f\"Model saved to {path}/{name}.pth\")\n",
    "    \n",
    "    return loss_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading and Model Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch (1/3): 100%|██████████| 10000/10000 [33:44<00:00,  4.94batch/s, NLL Loss=41.1075 | Reconstruction Loss: 0.7742]\n",
      "Epoch (2/3): 100%|██████████| 10000/10000 [35:49<00:00,  4.65batch/s, NLL Loss=35.1583 | Reconstruction Loss: 0.7241]\n",
      "Epoch (3/3): 100%|██████████| 10000/10000 [35:36<00:00,  4.68batch/s, NLL Loss=32.9845 | Reconstruction Loss: 0.7030]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to memory.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from utils import VAE\n",
    "import torch \n",
    "\n",
    "# Set up device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Model dimensions\n",
    "LATENT_DIM = 32\n",
    "ACTION_DIM = 3\n",
    "\n",
    "# Load the vision component (VAE)\n",
    "vae_model = VAE(3, LATENT_DIM).to(device)\n",
    "vae_model.load_state_dict(torch.load('models/vision_32.pth', map_location=device))\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = CarRacingDataset(h5_path='car_racing_data_10k.h5', transform=None, vae=vae_model, device=device)\n",
    "dataloader = DataLoader(dataset, shuffle=True, batch_size=1)\n",
    "\n",
    "# Training hyperparameters\n",
    "EPOCHS = 5\n",
    "TAU = 1.0\n",
    "NUM_GAUSSIAN = 5\n",
    "HIDDEN_DIM = 256\n",
    "\n",
    "# Initialize model and optimizer\n",
    "model = MDNRNN(latent_dim=32, action_dim=3, hidden_dim=256, num_gaussians=5).to(device)\n",
    "# Uncomment to load a pre-trained model\n",
    "# model.load_state_dict(torch.load('models/memory_256.pth', map_location=device))\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Train the model\n",
    "loss_history = train_model(model, dataloader, optimizer, num_epochs=EPOCHS, \n",
    "                          device=device, save=True, tau=TAU, name='memory_256')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Load model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = MDNRNN(latent_dim=32, action_dim=3, hidden_dim=256, num_gaussians=5).to(device)\n",
    "model.load_state_dict(torch.load('checkpoints/memory.pth', weights_only=False))\n",
    "\n",
    "\n",
    "from models.vae import VAE\n",
    "vae_model = VAE(3, 32).to(device)\n",
    "vae_model.load_state_dict(torch.load('checkpoints/vae.pth', weights_only=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4. Model Visualization**\n",
    "---\n",
    "\n",
    "### Interactive Visualization\n",
    "\n",
    "This function allows visual interaction with the model predictions in the Car Racing environment:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import pygame\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "import cv2\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Lambda(lambda img: img[:-12, :, :]),  # Crop the bottom 12 pixels\n",
    "    transforms.ToPILImage(),                         # Convert to PIL image\n",
    "    transforms.Resize((96, 96), transforms.InterpolationMode.LANCZOS),  # Resize\n",
    "    transforms.ToTensor()                            # Convert to tensor and scale to [0,1]\n",
    "])\n",
    "\n",
    "def run_car_racing_rnn_mda(env_name, vae_model, mdnrnn, transform, device, scale=1, resolution=(150, 150), \n",
    "                           tau=1.0, save_video=False, video_filename=\"car_racing_rnn.avi\"):\n",
    "    \"\"\"\n",
    "    Run the Car Racing environment with MDN-RNN visualization.\n",
    "    \n",
    "    Args:\n",
    "        env_name: Name of the Gym environment\n",
    "        vae_model: Trained VAE model\n",
    "        mdnrnn: Trained MDN-RNN model\n",
    "        transform: Image transform function\n",
    "        device: Device to run models on\n",
    "        scale: Display scale factor\n",
    "        resolution: Base resolution\n",
    "        tau: Temperature parameter for sampling\n",
    "        save_video: Whether to save a video of the run\n",
    "        video_filename: Filename for the output video\n",
    "    \"\"\"\n",
    "    # Initialize pygame for rendering\n",
    "    pygame.init()\n",
    "    resolution = (resolution[0] * 2 * scale, resolution[1] * scale)\n",
    "    screen = pygame.display.set_mode(resolution)\n",
    "    clock = pygame.time.Clock()  \n",
    "\n",
    "    os.makedirs('videos', exist_ok=True)\n",
    "    video_filename = os.path.join('videos', video_filename)\n",
    "    \n",
    "    action = np.zeros(3)  # Initialize action array\n",
    "    \n",
    "    def get_action(keys):\n",
    "        \"\"\" Map keyboard input to actions \"\"\"\n",
    "        action[0] = -1.0 if keys[pygame.K_LEFT] else 1.0 if keys[pygame.K_RIGHT] else 0.0  # Steering\n",
    "        action[1] = 1.0 if keys[pygame.K_UP] else 0.0  # Accelerate\n",
    "        action[2] = 1.0 if keys[pygame.K_DOWN] else 0.0  # Brake\n",
    "        return action\n",
    "    \n",
    "    # Initialize the environment\n",
    "    env = gym.make(env_name, render_mode='rgb_array')\n",
    "    obs, _ = env.reset()\n",
    "    \n",
    "    video_writer = None\n",
    "    if save_video:\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "        video_writer = cv2.VideoWriter(video_filename, fourcc, 30.0, (resolution[0], resolution[1]))\n",
    "\n",
    "    running = True\n",
    "    h = mdnrnn.rnn.init_hidden(1) \n",
    "    h = (h[0].to(device), h[1].to(device))\n",
    "    \n",
    "    cnt = 0\n",
    "    while running:\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                running = False\n",
    "        \n",
    "        keys = pygame.key.get_pressed()  # Get current key states\n",
    "        action = get_action(keys)        # Update action based on key presses\n",
    "\n",
    "        # Environment step \n",
    "        obs, reward, done, info, _ = env.step(action)\n",
    "        \n",
    "        # Render and process the frame\n",
    "        obs_tensor = transform(obs).unsqueeze(0).to(device)  # Transform frame to tensor\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Encode the frame using VAE\n",
    "            mu, logvar = vae_model.encode(obs_tensor)\n",
    "            z = vae_model.reparameterize(mu, logvar)  # Reparameterization trick\n",
    "                \n",
    "            # Generate predicted next image using RNN-MDN\n",
    "            action_tensor = torch.tensor(action, dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(device)\n",
    "\n",
    "            pi, mu, sigma, h = mdnrnn(z.unsqueeze(0), action_tensor, h=h, tau=tau)\n",
    "            z_next = sample_mdn(pi, mu, sigma)\n",
    "            \n",
    "            # Decode MDN sampled latent vector\n",
    "            reconstructed = vae_model.decode(z_next.squeeze(0))\n",
    "\n",
    "        cnt+=1\n",
    "        \n",
    "        # Prepare images for display\n",
    "        reconstructed = (reconstructed.squeeze(0).permute(2, 1, 0).cpu().numpy() * 255).astype(np.uint8)\n",
    "        obs = (obs_tensor.squeeze(0).permute(2, 1, 0).cpu().numpy() * 255).astype(np.uint8)\n",
    "        \n",
    "        # Concatenate original and reconstructed images\n",
    "        full_image = np.concatenate((obs, reconstructed), axis=0)\n",
    "        full_image_resized = cv2.resize(full_image, (resolution[1], resolution[0]), interpolation=cv2.INTER_LINEAR)\n",
    "        \n",
    "        if save_video and video_writer is not None:\n",
    "            video_writer.write(cv2.cvtColor(full_image_resized.transpose(1, 0, 2), cv2.COLOR_RGB2BGR))\n",
    "\n",
    "        # Display the combined image\n",
    "        clock.tick(30)\n",
    "        pygame.surfarray.blit_array(screen, full_image_resized)\n",
    "        pygame.display.flip()\n",
    "        \n",
    "        if done:\n",
    "            obs, _ = env.reset()  # Reset environment if done\n",
    "            h = mdnrnn.rnn.init_hidden(1) \n",
    "            h = (h[0].to(device), h[1].to(device))\n",
    "    \n",
    "    if save_video and video_writer is not None:\n",
    "        video_writer.release()\n",
    "\n",
    "    pygame.quit()\n",
    "    env.close()\n",
    "\n",
    "\n",
    "# Run the visualization\n",
    "run_car_racing_rnn_mda(env_name=\"CarRacing-v3\", vae_model=vae_model, mdnrnn=model, \n",
    "                      transform=transform, device=device, scale=4, tau=0.1, save_video=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5. Model \"Dreams\": Exploring Autonomous Generation**\n",
    "---\n",
    "\n",
    "This visualization demonstrates the model's ability to generate its own predictions autonomously, beginning from a single real observation and then continuing with self-generated states:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import pygame\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "import cv2\n",
    "\n",
    "def run_car_racing_rnn_mda(env_name, vae_model, mdnrnn, transform, device, scale=1, resolution=(96, 96), tau=1.0, save_video=False, video_filename=\"car_racing_dream.avi\"):\n",
    "    # Initialize the environment\n",
    "    env = gym.make(env_name, render_mode='rgb_array')\n",
    "    obs, _ = env.reset()\n",
    "    for _ in range(50):\n",
    "        env.step(np.array([0,0,0]))\n",
    "    # Initialize pygame for rendering\n",
    "    pygame.init()\n",
    "    resolution = (resolution[0] * scale, resolution[1] * scale)\n",
    "    screen = pygame.display.set_mode(resolution)\n",
    "\n",
    "    os.makedirs('videos', exist_ok=True)\n",
    "    video_filename = os.path.join('videos', video_filename)\n",
    "    \n",
    "    action = np.zeros(3)  # Initialize action array\n",
    "    \n",
    "    def get_action(keys):\n",
    "        \"\"\" Map keyboard input to actions \"\"\"\n",
    "        action[0] = -1.0 if keys[pygame.K_LEFT] else 1.0 if keys[pygame.K_RIGHT] else 0.0  # Steering\n",
    "        action[1] = 1.0 if keys[pygame.K_UP] else 0.0  # Accelerate\n",
    "        action[2] = 1.0 if keys[pygame.K_DOWN] else 0.0  # Brake\n",
    "        return action\n",
    "    \n",
    "    video_writer = None\n",
    "    if save_video:\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "        video_writer = cv2.VideoWriter(video_filename, fourcc, 30.0, (resolution[0], resolution[1]))\n",
    "\n",
    "    running = True\n",
    "    h = mdnrnn.rnn.init_hidden(1) \n",
    "    h = (h[0].to(device), h[1].to(device))\n",
    "    \n",
    "    cnt = 0\n",
    "    while running:\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                running = False\n",
    "        \n",
    "        keys = pygame.key.get_pressed()  # Get current key states\n",
    "        action = get_action(keys)        # Update action based on key presses\n",
    "        \n",
    "        ## Enviroment step \n",
    "        obs, reward, done, info, _ = env.step(action)\n",
    "        \n",
    "        obs_tensor = transform(obs).unsqueeze(0).to(device)  # Transform frame to tensor\n",
    "\n",
    "\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            # The fisrt latent vector comes from the enviroment\n",
    "            # the next is the previous z generated from the LSTM-MDN\n",
    "            if cnt ==0:\n",
    "                mu, logvar = vae_model.encode(obs_tensor)\n",
    "                z = vae_model.reparameterize(mu, logvar).unsqueeze(0) \n",
    "            else:\n",
    "                z = z_next\n",
    "                \n",
    "            # Generate predicted next image  using RNN-MDA\n",
    "            action_tensor = torch.tensor(action, dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(device)\n",
    "\n",
    "            pi, mu, sigma, h = mdnrnn(z, action_tensor, h=h, tau=tau)\n",
    "            z_next = sample_mdn(pi, mu, sigma)\n",
    "            \n",
    "            # Decode MDN sampled latent vector\n",
    "            reconstructed = vae_model.decode(z_next.squeeze(0))\n",
    "\n",
    "        cnt += 1\n",
    "        \n",
    "        # Prepare the reconstructed image for display\n",
    "        reconstructed = (reconstructed.squeeze(0).permute(2, 1, 0).cpu().numpy() * 255).astype(np.uint8)\n",
    "        reconstructed_resized = cv2.resize(reconstructed, (resolution[0], resolution[1]))\n",
    "\n",
    "        if save_video and video_writer is not None:\n",
    "            video_writer.write(cv2.cvtColor(reconstructed_resized.transpose(1, 0, 2), cv2.COLOR_RGB2BGR))\n",
    "\n",
    "        # Display only the reconstructed image\n",
    "        pygame.surfarray.blit_array(screen, reconstructed_resized)\n",
    "        pygame.display.flip()\n",
    "        \n",
    "        if done:\n",
    "            obs = env.reset()  # Reset environment if done\n",
    "            h = mdnrnn.rnn.init_hidden(1) \n",
    "            h = (h[0].to(device), h[1].to(device))\n",
    "    \n",
    "    if save_video and video_writer is not None:\n",
    "        video_writer.release()\n",
    "\n",
    "    pygame.quit()\n",
    "    env.close()\n",
    "\n",
    "\n",
    "run_car_racing_rnn_mda(env_name=\"CarRacing-v3\", vae_model=vae_model, mdnrnn=model, transform=transform, device=device, scale=6, tau=0.01, save_video=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
