{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Animations***\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Wrapper**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import torch\n",
    "import numpy as np\n",
    "import pygame\n",
    "\n",
    "class WorldModelEnv(gym.Wrapper):\n",
    "    def __init__(self, env_id='CarRacing-v3', vision_model=None, memory_model=None, device='cuda', render_mode='rgb_array'):\n",
    "        super().__init__(gym.make(env_id, lap_complete_percent=1.0, render_mode=render_mode))\n",
    "        \n",
    "        self.device = device\n",
    "        self.vision_model = vision_model\n",
    "        self.memory_model = memory_model\n",
    "        \n",
    "        self.latent_dim = vision_model.latent_dim\n",
    "        self.hidden_dim = memory_model.hidden_dim\n",
    "        \n",
    "        self.observation_space = spaces.Box(\n",
    "            low=-np.inf, \n",
    "            high=np.inf,\n",
    "            shape=(self.latent_dim + self.hidden_dim,),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "        \n",
    "        self.action_space = spaces.Box(\n",
    "            low=np.array([-1.0, 0.0, 0.0]),\n",
    "            high=np.array([1.0, 1.0, 1.0]),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "        \n",
    "        self.hidden_state = None\n",
    "        self.z = None\n",
    "\n",
    "        # Inicializamos pygame\n",
    "        pygame.init()\n",
    "        self.screen = None  # Se creará al llamar a render()\n",
    "        self.clock = pygame.time.Clock()\n",
    "        \n",
    "    def reset(self, **kwargs):\n",
    "        obs, info = super().reset(**kwargs)\n",
    "        \n",
    "        # Initial hidden state\n",
    "        self.hidden_state = self.memory_model.rnn.init_hidden(1, self.device)\n",
    "        \n",
    "        # Get latent space\n",
    "        self.z = self._encode_obs(obs)\n",
    "        \n",
    "        # Concatenates z and actual h\n",
    "        full_state = self._get_full_state(self.z)\n",
    "        \n",
    "        return full_state, info\n",
    "    \n",
    "    def step(self, action):\n",
    "        obs, reward, terminated, truncated, info = super().step(action)\n",
    "        \n",
    "        self.z = self._encode_obs(obs)\n",
    "        \n",
    "        # Update the hidden state with the MDNRNN\n",
    "        with torch.no_grad():\n",
    "            z_tensor = self.z.unsqueeze(1) \n",
    "            action_tensor = torch.tensor(action, dtype=torch.float32).unsqueeze(0).unsqueeze(1).to(self.device)\n",
    "            _, self.hidden_state = self.memory_model.rnn(z_tensor, action_tensor, self.hidden_state)\n",
    "        \n",
    "        # Concatenates z and actual h\n",
    "        full_state = self._get_full_state(self.z)\n",
    "        \n",
    "        return full_state, reward, terminated, truncated, info\n",
    "    \n",
    "\n",
    "        \n",
    "    def _encode_obs(self, obs, size=(96, 96), cropp=12):\n",
    "        \"\"\" Preprocess the image and encode it with VAE\"\"\"\n",
    "        with torch.no_grad():\n",
    "            obs_tensor = torch.tensor(obs, dtype=torch.float32).unsqueeze(0).to(self.device)\n",
    "            obs_tensor = obs_tensor.permute(0, 3, 1, 2) / 255.0\n",
    "            obs_tensor = obs_tensor[:, :, :-cropp, :]\n",
    "            obs_tensor = torch.nn.functional.interpolate(obs_tensor, size=size, mode='bicubic')\n",
    "            \n",
    "            mu, logvar = self.vision_model.encode(obs_tensor)\n",
    "            z = self.vision_model.reparameterize(mu, logvar)\n",
    "            \n",
    "        return z\n",
    "    \n",
    "    \n",
    "    def _decode_obs(self, z, size=(600,600)):\n",
    "        \"\"\"Decodes the latent state and reconstructs the image\"\"\"\n",
    "        with torch.no_grad():\n",
    "            reconstructed_image = self.vision_model.decode(z)\n",
    "            reconstructed_image = torch.nn.functional.interpolate(reconstructed_image, size=size, mode='bicubic').squeeze(0)\n",
    "            reconstructed_image = reconstructed_image.permute(1, 2, 0).cpu().numpy()\n",
    "            reconstructed_image = (reconstructed_image * 255).astype(np.uint8)\n",
    "        \n",
    "        return reconstructed_image\n",
    "    \n",
    "    def render_vae(self):\n",
    "        return self._decode_obs(self.z)\n",
    "    \n",
    "    def _get_full_state(self, z):\n",
    "        \"\"\"Concatenates the latent state with the hidden state\"\"\"\n",
    "        h = self.hidden_state[0].squeeze(0)\n",
    "        full_state = torch.cat([z, h], dim=-1)\n",
    "        return full_state.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bruno/Documents/TFG/.venv/lib/python3.12/site-packages/gymnasium/spaces/box.py:235: UserWarning: \u001b[33mWARN: Box low's precision lowered by casting to float32, current low.dtype=float64\u001b[0m\n",
      "  gym.logger.warn(\n",
      "/home/bruno/Documents/TFG/.venv/lib/python3.12/site-packages/gymnasium/spaces/box.py:305: UserWarning: \u001b[33mWARN: Box high's precision lowered by casting to float32, current high.dtype=float64\u001b[0m\n",
      "  gym.logger.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from models.controller import Controller\n",
    "from models.mdnrnn import MDNRNN\n",
    "from models.vae import VAE\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "LATENT_DIM = 32\n",
    "ACTION_DIM = 3\n",
    "HIDDEN_DIM = 256\n",
    "NUM_GAUSSIANS = 5\n",
    "\n",
    "path = 'checkpoints'\n",
    "# VAE model\n",
    "vision = VAE(3, LATENT_DIM).to(device)\n",
    "vision.load_state_dict(torch.load(f'{path}/vae.pth', weights_only=False))\n",
    "vision.eval()\n",
    "\n",
    "# LSTM-MDN model\n",
    "memory = MDNRNN(latent_dim=LATENT_DIM, action_dim=3, hidden_dim=HIDDEN_DIM, num_gaussians=NUM_GAUSSIANS).to(device)\n",
    "memory.load_state_dict(torch.load(f'{path}/memory.pth', weights_only=False))\n",
    "memory.eval()\n",
    "\n",
    "\n",
    "controller = Controller(LATENT_DIM+HIDDEN_DIM, ACTION_DIM).to(device)\n",
    "controller.load_state_dict(torch.load(f'{path}/controller.pth', weights_only=False))\n",
    "\n",
    "env =  WorldModelEnv(env_id='CarRacing-v3', device=device, vision_model=vision, memory_model=memory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **CMA-ES Parallel Animation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from tqdm import tqdm\n",
    "\n",
    "env =  WorldModelEnv(env_id='CarRacing-v3', device=device, vision_model=vision, memory_model=memory)\n",
    "render_dataset = []\n",
    "\n",
    "\n",
    "for episode in tqdm(range(16)):\n",
    "    episode_images = []\n",
    "    obs, info = env.reset()\n",
    "    for _ in range(1000):\n",
    "        img = env.render()\n",
    "        img = cv2.resize(img[:-50, :, :], (300,200))\n",
    "        img = np.array(img, dtype=np.uint8)\n",
    "        episode_images.append(img)\n",
    "        \n",
    "        action = controller.get_action(torch.tensor(obs, dtype=torch.float32).to(device))\n",
    "        obs, reward, terminated, truncated, info = env.step(action.cpu().numpy())\n",
    "    \n",
    "    render_dataset.append(episode_images)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video creado en: render_cma_train.mp4\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def create_combined_video(render_dataset, output_path='combined_video.mp4', fps=30):\n",
    "    \"\"\"\n",
    "    Combina los 16 episodios en un único video que muestra los 16 episodios al mismo tiempo en una cuadrícula 4x4.\n",
    "\n",
    "    Args:\n",
    "    - render_dataset (list): Lista de episodios, cada uno es una lista de imágenes.\n",
    "    - output_path (str): Ruta de salida para el archivo de video.\n",
    "    - fps (int): Cuadros por segundo del video.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Verifica que hay 16 episodios y obtén el tamaño del primer frame\n",
    "    assert len(render_dataset) == 16, \"render_dataset debe contener 16 episodios.\"\n",
    "    frame_shape = render_dataset[0][0].shape  # Asume que todos los episodios tienen el mismo tamaño de frame\n",
    "\n",
    "    # Configura la resolución del video final (cuadrícula 4x4)\n",
    "    frame_height, frame_width, _ = frame_shape\n",
    "    grid_height = frame_height * 4\n",
    "    grid_width = frame_width * 4\n",
    "    \n",
    "    # Configura el escritor de video\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (grid_width, grid_height))\n",
    "\n",
    "    # Itera a través de cada frame en el episodio\n",
    "    num_frames = len(render_dataset[0])  # Número de frames en cada episodio\n",
    "    for frame_idx in range(num_frames):\n",
    "        # Crea una cuadrícula de frames (4x4)\n",
    "        grid_frame = np.zeros((grid_height, grid_width, 3), dtype=np.uint8)\n",
    "        \n",
    "        for i in range(4):\n",
    "            for j in range(4):\n",
    "                # Calcula el índice del episodio correspondiente\n",
    "                episode_idx = i * 4 + j\n",
    "                # Obtiene el frame actual del episodio y convierte a RGB\n",
    "                episode_frame = cv2.cvtColor(render_dataset[episode_idx][frame_idx], cv2.COLOR_BGR2RGB)\n",
    "                # Coloca el frame en la cuadrícula\n",
    "                y_start, y_end = i * frame_height, (i + 1) * frame_height\n",
    "                x_start, x_end = j * frame_width, (j + 1) * frame_width\n",
    "                grid_frame[y_start:y_end, x_start:x_end] = episode_frame\n",
    "\n",
    "        # Escribe el frame de la cuadrícula en el video\n",
    "        out.write(grid_frame)\n",
    "    \n",
    "    # Libera el escritor de video\n",
    "    out.release()\n",
    "    print(f\"Video creado en: {output_path}\")\n",
    "\n",
    "# Uso:\n",
    "create_combined_video(render_dataset, output_path='videos/cma_training.mp4', fps=30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
