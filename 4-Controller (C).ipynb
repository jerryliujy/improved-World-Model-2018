{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b774dcc-4649-440f-8fb7-9e1f79d64f73",
   "metadata": {},
   "source": [
    "# **World Models - Controller (C)**\n",
    "\n",
    "\n",
    "## **Introduction**\n",
    "\n",
    "This notebook implements the Controller (C) component of the World Models architecture. In the World Model framework, the agent consists of three components:\n",
    "* **Vision (V)**: A Variational Autoencoder (VAE) that compresses visual input into a latent representation\n",
    "* **Memory (M)**: A recurrent network (LSTM-MDN) that predicts future states\n",
    "* **Controller (C)**: A neural network that takes latent state and hidden state to output actions\n",
    "\n",
    "The Controller is trained using an evolutionary strategy (CMA-ES) to optimize performance in the CarRacing environment.\n",
    "\n",
    "\n",
    "<img src=\"imgs/controller.png\" width=\"600\" alt=\"World Models Architecture Diagram\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1565647",
   "metadata": {},
   "source": [
    "### **Loading Pre-trained Vision (VAE) and Memory (MDN-LSTM) Models**\n",
    "\n",
    "First, we load the pre-trained VAE and LSTM-MDN models that form the Vision and Memory components of our World Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3329aa9d-7f03-412e-b95e-4ff92cfc3fb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MDNRNN(\n",
       "  (rnn): RNN(\n",
       "    (lstm): LSTM(35, 256, batch_first=True)\n",
       "  )\n",
       "  (mdn): MDN(\n",
       "    (fc_pi): Linear(in_features=256, out_features=5, bias=True)\n",
       "    (fc_mu): Linear(in_features=256, out_features=160, bias=True)\n",
       "    (fc_sigma): Linear(in_features=256, out_features=160, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from models.vae import VAE\n",
    "from models.mdnrnn import MDNRNN\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "LATENT_DIM = 32\n",
    "ACTION_DIM = 3\n",
    "HIDDEN_DIM = 256\n",
    "NUM_GAUSSIANS = 5\n",
    "\n",
    "# VAE model\n",
    "vision = VAE(3, LATENT_DIM).to(device)\n",
    "vision.load_state_dict(torch.load('checkpoints/vae.pth', weights_only=False))\n",
    "vision.eval()\n",
    "\n",
    "# LSTM-MDN model\n",
    "memory = MDNRNN(latent_dim=LATENT_DIM, action_dim=3, hidden_dim=HIDDEN_DIM, num_gaussians=NUM_GAUSSIANS).to(device)\n",
    "memory.load_state_dict(torch.load('checkpoints/memory.pth', weights_only=False))\n",
    "memory.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0448edc4",
   "metadata": {},
   "source": [
    "## **Controller Architecture**\n",
    "---\n",
    "\n",
    "The Controller receives the latent vector `z` from the VAE and the hidden state `h` from the LSTM-MDN model, and outputs the optimal action to take.\n",
    "\n",
    "In the CarRacing environment, the controller produces **three continuous actions**:\n",
    "- **Steering**: Range [-1, 1] (using tanh activation)\n",
    "- **Acceleration**: Range [0, 1] (using sigmoid activation)\n",
    "- **Brake**: Range [0, 1] (using sigmoid activation, limited to 0.8 max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35bb787c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Controller(nn.Module):\n",
    "    \"\"\"\n",
    "    Controller neural network that maps state (z + h) to actions.\n",
    "    \n",
    "    Args:\n",
    "        state_dim (int): Dimension of the input state (latent + hidden state)\n",
    "        action_dim (int): Dimension of the output action space\n",
    "    \"\"\"\n",
    "    def __init__(self, state_dim, action_dim=3):\n",
    "        super(Controller, self).__init__()\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(state_dim, action_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the controller network.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor containing latent state + hidden state\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: Action values for steering, acceleration, and brake\n",
    "        \"\"\"\n",
    "        raw_actions = self.model(x)\n",
    "        \n",
    "        # Apply appropriate activations to each action dimension\n",
    "        steering = torch.tanh(raw_actions[:, 0:1])        # [-1, 1]\n",
    "        gas = torch.sigmoid(raw_actions[:, 1:2])          # [0, 1]\n",
    "        brake = torch.sigmoid(raw_actions[:, 2:3]) * 0.8  # [0, 0.8]\n",
    "\n",
    "        # Ensure brake is reduced when accelerating (realistic vehicle behavior)\n",
    "        brake = brake * (1-gas)\n",
    "        \n",
    "        # Combine all actions into one tensor\n",
    "        actions = torch.cat([steering, gas, brake], dim=1)\n",
    "    \n",
    "        return actions\n",
    "\n",
    "    def get_action(self, state):\n",
    "        \"\"\"\n",
    "        Get action for a given state without gradient computation.\n",
    "        \n",
    "        Args:\n",
    "            state (torch.Tensor): Current state (z + h)\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: Action to take\n",
    "        \"\"\"\n",
    "        with torch.no_grad():  \n",
    "            action = self.forward(state)\n",
    "        return action.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9546f81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Controller(\n",
      "  (model): Sequential(\n",
      "    (0): Linear(in_features=288, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "Num params: 867\n"
     ]
    }
   ],
   "source": [
    "controller = Controller(state_dim = LATENT_DIM + HIDDEN_DIM, action_dim=ACTION_DIM).to(device)\n",
    "\n",
    "print(controller)\n",
    "print(f'Num params: {sum(p.numel() for p in controller.parameters())}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22462245",
   "metadata": {},
   "source": [
    "## **CMA-ES: Covariance Matrix Adaptation Evolution Strategy**\n",
    "---\n",
    "\n",
    "### Theoretical Background\n",
    "\n",
    "CMA-ES is a powerful evolutionary algorithm designed for challenging non-linear, non-convex optimization problems. It's particularly effective for training neural networks in reinforcement learning settings where gradient-based methods may struggle.\n",
    "\n",
    "**Key Principles of CMA-ES:**\n",
    "\n",
    "1. **Initialization**: The algorithm starts with a multivariate normal distribution defined by an initial **mean** (representing the solution) and a **covariance matrix** which controls exploration.\n",
    "\n",
    "2. **Population Generation**: In each iteration, CMA-ES generates a population of candidate solutions by sampling from the current distribution. The scale of variation is controlled by the **sigma** (step size) parameter.\n",
    "\n",
    "3. **Fitness Evaluation**: Each candidate solution is evaluated on the target problem to determine its fitness.\n",
    "\n",
    "4. **Distribution Update**: The algorithm updates:\n",
    "   - The mean vector (shifting toward better solutions)\n",
    "   - The covariance matrix (adapting the search distribution to favor promising directions)\n",
    "   - The step size (controlling the overall scale of exploration)\n",
    "\n",
    "### Application to World Models\n",
    "\n",
    "In our implementation, CMA-ES optimizes the **weights of the controller neural network**. The controller maps latent states `z` and hidden states `h` to actions `a`.\n",
    "\n",
    "Since the CarRacing environment has randomized tracks, each controller's performance varies depending on the specific track it encounters. To provide a more reliable fitness evaluation, **each controller is evaluated multiple times** (on different tracks), and the average reward is used as the fitness measure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9029c22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.vector import AsyncVectorEnv\n",
    "\n",
    "def make_env(name='CarRacing-v3'):\n",
    "    \"\"\"\n",
    "    Factory function to create wrapped environment.\n",
    "    \n",
    "    Args:\n",
    "        name (str): Gymnasium environment name\n",
    "        \n",
    "    Returns:\n",
    "        callable: Function that creates the specified environment\n",
    "    \"\"\"\n",
    "    def _init():\n",
    "        env = gym.make(name, render_mode='rgb_array', \n",
    "                       lap_complete_percent=1.0,\n",
    "                       domain_randomize=False, continuous=True)\n",
    "        return env\n",
    "    return _init\n",
    "\n",
    "def create_vector_envs(num_envs):\n",
    "    \"\"\"\n",
    "    Create vectorized environments for parallel execution.\n",
    "    \n",
    "    Args:\n",
    "        num_envs (int): Number of parallel environments\n",
    "        \n",
    "    Returns:\n",
    "        AsyncVectorEnv: Vectorized environment instance\n",
    "    \"\"\"\n",
    "    return AsyncVectorEnv([make_env() for _ in range(num_envs)], \n",
    "                          shared_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f7ed82",
   "metadata": {},
   "source": [
    "## **Batch Processing for Efficient Evaluation**\n",
    "---\n",
    "\n",
    "We implement batch processing to efficiently evaluate multiple controller instances in parallel:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35c2a952-7274-42f3-8404-30c1f4b0afe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def encode_obs_batch(obs_batch, size=(96, 96), device='cuda'):\n",
    "    \"\"\"\n",
    "    Preprocess and encode a batch of observations to latent vectors.\n",
    "    \n",
    "    Args:\n",
    "        obs_batch (numpy.ndarray): Batch of observations from environments\n",
    "        size (tuple): Target size for image preprocessing\n",
    "        device (str): Device for computation ('cuda' or 'cpu')\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: Batch of latent vectors\n",
    "    \"\"\"\n",
    "    # Convert to tensor and normalize\n",
    "    obs_tensor = torch.as_tensor(obs_batch, dtype=torch.float32, device=device)\n",
    "    obs_tensor = obs_tensor.permute(0, 3, 1, 2) / 255.0\n",
    "    \n",
    "    # Crop and resize images\n",
    "    obs_tensor = obs_tensor[:, :, :-12, :]  # Remove bottom status bar\n",
    "    obs_tensor = F.interpolate(obs_tensor, size=size, mode='bicubic')\n",
    "    \n",
    "    # Encode to latent space\n",
    "    with torch.no_grad():\n",
    "        mu, logvar = vision.encode(obs_tensor)\n",
    "        z = vision.reparameterize(mu, logvar)\n",
    "    return z\n",
    "\n",
    "\n",
    "def decode_obs(z):\n",
    "    \"\"\"\n",
    "    Decode latent vector to reconstructed observation.\n",
    "    \n",
    "    Args:\n",
    "        z (torch.Tensor): Latent vector\n",
    "        \n",
    "    Returns:\n",
    "        numpy.ndarray: Reconstructed image as numpy array\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        obs_recon = vision.decode(z)\n",
    "    return obs_recon.squeeze(0).permute(1, 2, 0).cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c96e532e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_actions(controllers, x):\n",
    "    \"\"\"\n",
    "    Process actions for all controllers at once in one generation.\n",
    "    \n",
    "    Args:\n",
    "        controllers (list): List of controller models\n",
    "        x (torch.Tensor): Batch of input states\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: Batch of actions\n",
    "    \"\"\"\n",
    "    return torch.stack([controllers[i].get_action(x[i:i+1]) \n",
    "                        for i in range(x.size(0))], dim=0)\n",
    "\n",
    "def load_weights(controller_class, solutions):\n",
    "    \"\"\"\n",
    "    Load weights (CMA-ES solutions) for each controller in a generation.\n",
    "    \n",
    "    Args:\n",
    "        controller_class: Controller class to instantiate\n",
    "        solutions (list): List of parameter vectors for controllers\n",
    "        \n",
    "    Returns:\n",
    "        list: List of controller instances with loaded weights\n",
    "    \"\"\"\n",
    "    controllers = []\n",
    "    with torch.no_grad():\n",
    "        for params in solutions:\n",
    "            ctrl = controller_class(state_dim=LATENT_DIM + HIDDEN_DIM, \n",
    "                                    action_dim=ACTION_DIM).to(device)\n",
    "            torch.nn.utils.vector_to_parameters(\n",
    "                torch.tensor(params, dtype=torch.float32).to(device), \n",
    "                ctrl.parameters()\n",
    "            )\n",
    "            controllers.append(ctrl)\n",
    "    return controllers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5a5dbcb-60de-43bd-9177-7cb355b3900b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def evaluate_policies(solutions, controller_class, max_steps, memory):\n",
    "    \"\"\"\n",
    "    Evaluate multiple policies in parallel using vectorized environments.\n",
    "    \n",
    "    Args:\n",
    "        solutions (list): List of parameter vectors for controllers\n",
    "        controller_class: Controller class to instantiate\n",
    "        max_steps (int): Maximum number of steps per episode\n",
    "        memory: LSTM-MDN memory model\n",
    "        \n",
    "    Returns:\n",
    "        list: Cumulative rewards for each policy\n",
    "    \"\"\"\n",
    "    num_policies = len(solutions)\n",
    "    \n",
    "    # Create controllers with respective parameters\n",
    "    controllers = load_weights(controller_class, solutions)\n",
    "\n",
    "    # Create vectorized environments\n",
    "    envs = create_vector_envs(num_envs=num_policies)\n",
    "    obs, _ = envs.reset()\n",
    "\n",
    "    # Initialize hidden states for all policies\n",
    "    hidden = memory.rnn.init_hidden(num_policies, 'cuda')\n",
    "    \n",
    "    # Track rewards and completion status\n",
    "    cumulative_rewards = np.zeros(num_policies)\n",
    "    dones = np.full(num_policies, False)\n",
    "\n",
    "    # Episode rollout\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_steps):\n",
    "            \n",
    "            if np.all(dones):\n",
    "                # Stop if all environments are done\n",
    "                break\n",
    "\n",
    "            # Encode observations to latent space\n",
    "            z_batch = encode_obs_batch(obs)\n",
    "            \n",
    "            # Combine latent vectors with hidden states\n",
    "            h = hidden[0].squeeze(0)\n",
    "            x = torch.cat([z_batch, h], dim=-1)\n",
    "\n",
    "            # Get actions from controllers\n",
    "            actions = process_actions(controllers, x)\n",
    "            \n",
    "            # Step environments\n",
    "            obs, rewards, dones_new, _, _ = envs.step(actions.detach().cpu().numpy())\n",
    "            \n",
    "            # Update LSTM hidden states\n",
    "            z_batch = z_batch.unsqueeze(1)\n",
    "            actions = actions.unsqueeze(1)\n",
    "            _, hidden = memory.rnn(z_batch, actions, hidden)\n",
    "            \n",
    "            # Update rewards and done status\n",
    "            dones = np.logical_or(dones, dones_new)\n",
    "            cumulative_rewards += rewards * (~dones)\n",
    "            \n",
    "    envs.close()\n",
    "    return cumulative_rewards.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e97b992",
   "metadata": {},
   "source": [
    "## **CMA-ES Training Implementation**\n",
    "---\n",
    "\n",
    "Each controller was evaluated 16 times to account for track variability.\n",
    "\n",
    "Our implementation uses a modified approach:\n",
    "- Population size: 16 (matching available CPU cores)\n",
    "- Evaluations per controller: 7 (balance between reliability and computation time)\n",
    "- Initial sigma: 0.5 (exploration factor)\n",
    "- Sigma decay: 0.992 (gradually reducing exploration)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7cc9ab2-182f-49db-a9b9-e164c6af848a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cma\n",
    "import torch \n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "INITIAL_SIGMA = 0.1\n",
    "SIGMA_DECAY = 0.992\n",
    "np.random.seed(101)  # For reproducibility\n",
    "\n",
    "def train_cma_es(controller_class, memory, max_generations=100, max_steps=1000, \n",
    "                popsize=16, checkpoint=10, rollouts=7):\n",
    "    \"\"\"\n",
    "    Train controller using CMA-ES evolutionary strategy.\n",
    "    \n",
    "    Args:\n",
    "        controller_class: Controller class to train\n",
    "        memory: LSTM-MDN memory model\n",
    "        max_generations (int): Maximum number of generations to train\n",
    "        max_steps (int): Maximum steps per episode\n",
    "        popsize (int): Population size per generation\n",
    "        checkpoint (int): Save interval for checkpoints\n",
    "        rollouts (int): Number of evaluations per controller\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (es, metrics, best_solution)\n",
    "    \"\"\"\n",
    "    # Setup metrics tracking\n",
    "    metrics = {\n",
    "        'generation': [],\n",
    "        'best_reward': [],\n",
    "        'mean_reward': [],\n",
    "        'worst_reward': []\n",
    "    }\n",
    "    \n",
    "    # Create directory for checkpoints\n",
    "    os.makedirs('checkpoints/cma', exist_ok=True)\n",
    "\n",
    "    # Load existing model if continuing training, otherwise start fresh\n",
    "    controller = Controller(state_dim=LATENT_DIM + HIDDEN_DIM, \n",
    "                          action_dim=ACTION_DIM).to(device)\n",
    "    initial_params = torch.nn.utils.parameters_to_vector(\n",
    "        controller.parameters()).detach().cpu().numpy()\n",
    "\n",
    "    # Initialize CMA-ES optimizer\n",
    "    es = cma.CMAEvolutionStrategy(initial_params, INITIAL_SIGMA, {'popsize': popsize})\n",
    "    \n",
    "    # Main training loop\n",
    "    for generation in range(141, max_generations+1):\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Generate population for this generation\n",
    "        solutions = es.ask()  \n",
    "\n",
    "        # Evaluate each solution multiple times and average\n",
    "        mean_rewards = []\n",
    "        for _ in range(rollouts):\n",
    "            rewards = evaluate_policies(solutions, controller_class, max_steps, memory)\n",
    "            mean_rewards.append(rewards)\n",
    "        mean_rewards = np.mean(mean_rewards, axis=0)       \n",
    "\n",
    "        # Update CMA-ES with rewards (negative because CMA-ES minimizes)\n",
    "        es.tell(solutions, [-r for r in mean_rewards])\n",
    "        \n",
    "        # Calculate and log training statistics\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        minutes, seconds = divmod(elapsed_time, 60)\n",
    "        log = (f'Generation ({generation}/{max_generations}) | '\n",
    "               f'Best Reward: {round(np.max(mean_rewards))} | '\n",
    "               f'Avg Reward: {np.mean(mean_rewards):.2f} | '\n",
    "               f'Worst: {round(np.min(mean_rewards))} | '\n",
    "               f'Time: {int(minutes)}:{int(seconds):02d} | '\n",
    "               f'Sigma: {es.sigma:.4f}')\n",
    "        print(log)\n",
    "\n",
    "        # Update metrics\n",
    "        metrics['generation'].append(generation)\n",
    "        metrics['best_reward'].append(np.max(mean_rewards))\n",
    "        metrics['worst_reward'].append(np.min(mean_rewards))\n",
    "        metrics['mean_reward'].append(np.mean(mean_rewards))\n",
    "                \n",
    "        # Save metrics and best controller at checkpoints\n",
    "        if generation % checkpoint == 0:\n",
    "            best_controller = Controller(state_dim=LATENT_DIM + HIDDEN_DIM, \n",
    "                                       action_dim=ACTION_DIM).to(device)\n",
    "            torch.nn.utils.vector_to_parameters(\n",
    "                torch.tensor(es.result.xbest, dtype=torch.float32).to(device),\n",
    "                best_controller.parameters())\n",
    "            torch.save(best_controller.state_dict(), \n",
    "                      f'checkpoints/cma/controller_{generation}.pth')\n",
    "            pd.DataFrame(metrics).to_csv(f\"checkpoints/cma/cma_es_metrics.csv\")\n",
    "            print('--Checkpoint: best controller saved')\n",
    "    \n",
    "    # Save final model\n",
    "    best_controller = Controller(state_dim=LATENT_DIM + HIDDEN_DIM, \n",
    "                               action_dim=ACTION_DIM).to(device)\n",
    "    torch.nn.utils.vector_to_parameters(\n",
    "        torch.tensor(es.result.xbest, dtype=torch.float32).to(device),\n",
    "        best_controller.parameters())\n",
    "    torch.save(best_controller.state_dict(), f'checkpoints/controller.pth')\n",
    "    return es, metrics, es.result.xbest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "470ad830-41d5-482f-aa6f-7b9364891eb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8_w,16)-aCMA-ES (mu_w=4.8,w_1=32%) in dimension 867 (seed=601171, Wed Nov 13 08:49:56 2024)\n",
      "Generation (141/220) | Best Reward: 808 | AVG Reward: 666.44 | Worst 550 | Time: 3:35 | Sigma: 0.0993\n",
      "Generation (142/220) | Best Reward: 803 | AVG Reward: 672.96 | Worst 554 | Time: 3:38 | Sigma: 0.0987\n",
      "Generation (143/220) | Best Reward: 772 | AVG Reward: 666.39 | Worst 536 | Time: 3:39 | Sigma: 0.0981\n",
      "Generation (144/220) | Best Reward: 828 | AVG Reward: 692.14 | Worst 551 | Time: 3:33 | Sigma: 0.0975\n",
      "Generation (145/220) | Best Reward: 805 | AVG Reward: 699.01 | Worst 500 | Time: 3:33 | Sigma: 0.0970\n",
      "Generation (146/220) | Best Reward: 806 | AVG Reward: 694.74 | Worst 536 | Time: 3:32 | Sigma: 0.0965\n",
      "Generation (147/220) | Best Reward: 776 | AVG Reward: 711.61 | Worst 589 | Time: 3:33 | Sigma: 0.0960\n",
      "Generation (148/220) | Best Reward: 810 | AVG Reward: 693.91 | Worst 568 | Time: 3:33 | Sigma: 0.0955\n",
      "Generation (149/220) | Best Reward: 845 | AVG Reward: 726.06 | Worst 622 | Time: 3:33 | Sigma: 0.0950\n",
      "Generation (150/220) | Best Reward: 818 | AVG Reward: 710.69 | Worst 527 | Time: 3:33 | Sigma: 0.0946\n",
      "--Checkpoint: best controller saved\n",
      "Generation (151/220) | Best Reward: 802 | AVG Reward: 689.73 | Worst 524 | Time: 3:33 | Sigma: 0.0941\n",
      "Generation (152/220) | Best Reward: 801 | AVG Reward: 671.16 | Worst 510 | Time: 3:37 | Sigma: 0.0937\n",
      "Generation (153/220) | Best Reward: 812 | AVG Reward: 696.96 | Worst 614 | Time: 3:34 | Sigma: 0.0933\n",
      "Generation (154/220) | Best Reward: 786 | AVG Reward: 711.84 | Worst 619 | Time: 3:41 | Sigma: 0.0929\n",
      "Generation (155/220) | Best Reward: 840 | AVG Reward: 725.44 | Worst 598 | Time: 3:36 | Sigma: 0.0925\n",
      "Generation (156/220) | Best Reward: 820 | AVG Reward: 718.05 | Worst 593 | Time: 3:39 | Sigma: 0.0921\n",
      "Generation (157/220) | Best Reward: 790 | AVG Reward: 692.70 | Worst 593 | Time: 3:41 | Sigma: 0.0917\n",
      "Generation (158/220) | Best Reward: 787 | AVG Reward: 729.82 | Worst 658 | Time: 3:35 | Sigma: 0.0914\n",
      "Generation (159/220) | Best Reward: 811 | AVG Reward: 748.83 | Worst 630 | Time: 3:35 | Sigma: 0.0910\n",
      "Generation (160/220) | Best Reward: 809 | AVG Reward: 698.78 | Worst 596 | Time: 3:36 | Sigma: 0.0907\n",
      "--Checkpoint: best controller saved\n",
      "Generation (161/220) | Best Reward: 822 | AVG Reward: 722.04 | Worst 640 | Time: 3:38 | Sigma: 0.0904\n",
      "Generation (162/220) | Best Reward: 841 | AVG Reward: 731.84 | Worst 641 | Time: 3:34 | Sigma: 0.0900\n",
      "Generation (163/220) | Best Reward: 835 | AVG Reward: 735.69 | Worst 565 | Time: 3:34 | Sigma: 0.0897\n",
      "Generation (164/220) | Best Reward: 855 | AVG Reward: 746.77 | Worst 677 | Time: 3:35 | Sigma: 0.0894\n",
      "Generation (165/220) | Best Reward: 752 | AVG Reward: 696.91 | Worst 600 | Time: 3:35 | Sigma: 0.0891\n",
      "Generation (166/220) | Best Reward: 816 | AVG Reward: 716.72 | Worst 575 | Time: 3:36 | Sigma: 0.0888\n",
      "Generation (167/220) | Best Reward: 784 | AVG Reward: 725.83 | Worst 650 | Time: 3:35 | Sigma: 0.0885\n",
      "Generation (168/220) | Best Reward: 802 | AVG Reward: 726.64 | Worst 613 | Time: 3:34 | Sigma: 0.0883\n",
      "Generation (169/220) | Best Reward: 819 | AVG Reward: 735.16 | Worst 638 | Time: 3:33 | Sigma: 0.0880\n",
      "Generation (170/220) | Best Reward: 756 | AVG Reward: 704.59 | Worst 609 | Time: 3:34 | Sigma: 0.0877\n",
      "--Checkpoint: best controller saved\n",
      "Generation (171/220) | Best Reward: 784 | AVG Reward: 716.64 | Worst 652 | Time: 3:34 | Sigma: 0.0875\n",
      "Generation (172/220) | Best Reward: 806 | AVG Reward: 716.89 | Worst 608 | Time: 3:34 | Sigma: 0.0872\n",
      "Generation (173/220) | Best Reward: 803 | AVG Reward: 699.42 | Worst 559 | Time: 3:35 | Sigma: 0.0870\n",
      "Generation (174/220) | Best Reward: 821 | AVG Reward: 728.52 | Worst 653 | Time: 3:35 | Sigma: 0.0868\n",
      "Generation (175/220) | Best Reward: 815 | AVG Reward: 713.72 | Worst 589 | Time: 3:34 | Sigma: 0.0865\n",
      "Generation (176/220) | Best Reward: 810 | AVG Reward: 729.66 | Worst 651 | Time: 3:32 | Sigma: 0.0863\n",
      "Generation (177/220) | Best Reward: 807 | AVG Reward: 717.96 | Worst 581 | Time: 3:32 | Sigma: 0.0861\n",
      "Generation (178/220) | Best Reward: 796 | AVG Reward: 725.48 | Worst 664 | Time: 3:32 | Sigma: 0.0859\n",
      "Generation (179/220) | Best Reward: 791 | AVG Reward: 698.82 | Worst 560 | Time: 3:33 | Sigma: 0.0857\n",
      "Generation (180/220) | Best Reward: 814 | AVG Reward: 708.58 | Worst 630 | Time: 3:35 | Sigma: 0.0855\n",
      "--Checkpoint: best controller saved\n",
      "Generation (181/220) | Best Reward: 826 | AVG Reward: 717.10 | Worst 580 | Time: 3:36 | Sigma: 0.0853\n",
      "Generation (182/220) | Best Reward: 791 | AVG Reward: 701.28 | Worst 532 | Time: 3:34 | Sigma: 0.0851\n",
      "Generation (183/220) | Best Reward: 786 | AVG Reward: 712.30 | Worst 648 | Time: 3:34 | Sigma: 0.0849\n",
      "Generation (184/220) | Best Reward: 837 | AVG Reward: 713.55 | Worst 621 | Time: 3:33 | Sigma: 0.0847\n",
      "Generation (185/220) | Best Reward: 800 | AVG Reward: 717.87 | Worst 630 | Time: 3:34 | Sigma: 0.0845\n",
      "Generation (186/220) | Best Reward: 813 | AVG Reward: 722.08 | Worst 579 | Time: 3:33 | Sigma: 0.0843\n",
      "Generation (187/220) | Best Reward: 828 | AVG Reward: 722.00 | Worst 601 | Time: 3:34 | Sigma: 0.0841\n",
      "Generation (188/220) | Best Reward: 820 | AVG Reward: 738.74 | Worst 634 | Time: 3:33 | Sigma: 0.0840\n",
      "Generation (189/220) | Best Reward: 777 | AVG Reward: 705.62 | Worst 537 | Time: 3:33 | Sigma: 0.0838\n",
      "Generation (190/220) | Best Reward: 814 | AVG Reward: 704.11 | Worst 616 | Time: 3:34 | Sigma: 0.0836\n",
      "--Checkpoint: best controller saved\n",
      "Generation (191/220) | Best Reward: 804 | AVG Reward: 719.24 | Worst 623 | Time: 3:33 | Sigma: 0.0834\n",
      "Generation (192/220) | Best Reward: 807 | AVG Reward: 711.90 | Worst 572 | Time: 3:34 | Sigma: 0.0833\n",
      "Generation (193/220) | Best Reward: 777 | AVG Reward: 688.57 | Worst 545 | Time: 3:35 | Sigma: 0.0831\n",
      "Generation (194/220) | Best Reward: 806 | AVG Reward: 729.69 | Worst 655 | Time: 3:36 | Sigma: 0.0829\n",
      "Generation (195/220) | Best Reward: 798 | AVG Reward: 727.57 | Worst 612 | Time: 3:33 | Sigma: 0.0828\n",
      "Generation (196/220) | Best Reward: 830 | AVG Reward: 723.61 | Worst 651 | Time: 3:33 | Sigma: 0.0826\n",
      "Generation (197/220) | Best Reward: 782 | AVG Reward: 727.32 | Worst 658 | Time: 3:33 | Sigma: 0.0825\n",
      "Generation (198/220) | Best Reward: 813 | AVG Reward: 729.07 | Worst 596 | Time: 3:33 | Sigma: 0.0823\n",
      "Generation (199/220) | Best Reward: 808 | AVG Reward: 722.38 | Worst 478 | Time: 3:33 | Sigma: 0.0822\n",
      "Generation (200/220) | Best Reward: 816 | AVG Reward: 729.69 | Worst 595 | Time: 3:33 | Sigma: 0.0820\n",
      "--Checkpoint: best controller saved\n",
      "Generation (201/220) | Best Reward: 806 | AVG Reward: 716.07 | Worst 554 | Time: 3:33 | Sigma: 0.0819\n",
      "Generation (202/220) | Best Reward: 809 | AVG Reward: 721.35 | Worst 618 | Time: 3:35 | Sigma: 0.0818\n",
      "Generation (203/220) | Best Reward: 846 | AVG Reward: 698.87 | Worst 579 | Time: 3:36 | Sigma: 0.0816\n",
      "Generation (204/220) | Best Reward: 840 | AVG Reward: 745.39 | Worst 582 | Time: 3:34 | Sigma: 0.0815\n",
      "Generation (205/220) | Best Reward: 837 | AVG Reward: 712.01 | Worst 587 | Time: 3:33 | Sigma: 0.0814\n",
      "Generation (206/220) | Best Reward: 810 | AVG Reward: 702.15 | Worst 532 | Time: 3:34 | Sigma: 0.0813\n",
      "Generation (207/220) | Best Reward: 840 | AVG Reward: 759.62 | Worst 684 | Time: 3:34 | Sigma: 0.0811\n",
      "Generation (208/220) | Best Reward: 840 | AVG Reward: 732.62 | Worst 527 | Time: 3:34 | Sigma: 0.0810\n",
      "Generation (209/220) | Best Reward: 806 | AVG Reward: 730.25 | Worst 589 | Time: 3:33 | Sigma: 0.0809\n",
      "Generation (210/220) | Best Reward: 804 | AVG Reward: 719.58 | Worst 612 | Time: 3:34 | Sigma: 0.0808\n",
      "--Checkpoint: best controller saved\n",
      "Generation (211/220) | Best Reward: 842 | AVG Reward: 734.92 | Worst 642 | Time: 3:34 | Sigma: 0.0807\n",
      "Generation (212/220) | Best Reward: 808 | AVG Reward: 725.31 | Worst 540 | Time: 3:34 | Sigma: 0.0805\n",
      "Generation (213/220) | Best Reward: 837 | AVG Reward: 734.11 | Worst 583 | Time: 3:34 | Sigma: 0.0804\n",
      "Generation (214/220) | Best Reward: 826 | AVG Reward: 737.15 | Worst 600 | Time: 3:33 | Sigma: 0.0803\n",
      "Generation (215/220) | Best Reward: 858 | AVG Reward: 739.57 | Worst 660 | Time: 3:35 | Sigma: 0.0802\n",
      "Generation (216/220) | Best Reward: 838 | AVG Reward: 717.94 | Worst 640 | Time: 3:41 | Sigma: 0.0801\n",
      "Generation (217/220) | Best Reward: 812 | AVG Reward: 746.05 | Worst 678 | Time: 3:38 | Sigma: 0.0800\n",
      "Generation (218/220) | Best Reward: 814 | AVG Reward: 703.74 | Worst 540 | Time: 3:35 | Sigma: 0.0799\n",
      "Generation (219/220) | Best Reward: 803 | AVG Reward: 730.01 | Worst 662 | Time: 3:33 | Sigma: 0.0797\n",
      "Generation (220/220) | Best Reward: 818 | AVG Reward: 722.12 | Worst 595 | Time: 3:33 | Sigma: 0.0796\n",
      "--Checkpoint: best controller saved\n"
     ]
    }
   ],
   "source": [
    "es, metrics, best_solution = train_cma_es(Controller, memory, max_generations=220, max_steps=1000, popsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bfa9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "def hex_to_rgba(hex_color, alpha=0.2):\n",
    "    \"\"\"HEX to RGBA.\"\"\"\n",
    "    hex_color = hex_color.lstrip('#')\n",
    "    return f'rgba({int(hex_color[0:2],16)}, {int(hex_color[2:4],16)}, {int(hex_color[4:6],16)}, {alpha})'\n",
    "\n",
    "import plotly.io as pio\n",
    "\n",
    "def plot_cma_es_results(metrics, colors=['#86D293', '#FFCF96', '#FF8080'], metric_columns=['best_reward', 'mean_reward', 'worst_reward']):\n",
    "    fig = go.Figure()\n",
    "\n",
    "    for metric, color in zip(metric_columns, colors):\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=metrics['generation'],\n",
    "            y=metrics[metric],\n",
    "            mode='lines',\n",
    "            name=metric.replace('_', ' ').title(),\n",
    "            line=dict(color=color, width=2.5),\n",
    "            fill='tozeroy',\n",
    "            fillcolor=hex_to_rgba(color, 0.2)\n",
    "        ))\n",
    "\n",
    "    fig.update_layout(\n",
    "        height=600,\n",
    "        width=900,\n",
    "        plot_bgcolor='white',\n",
    "        paper_bgcolor='white',\n",
    "        font=dict(family=\"Helvetica, Arial, sans-serif\",size=14,color=\"#333333\"),\n",
    "        title=dict(text='CMA-ES Training Results',x=0.5,y=0.95,xanchor='center',yanchor='top',font=dict(size=20,color=\"#333333\")),\n",
    "        legend=dict(title='',title_font_size=16,font_size=14,bgcolor='rgba(255,255,255,0)',bordercolor='rgba(0,0,0,0)',orientation='h',yanchor='bottom',y=1.02,xanchor='center',x=0.5),\n",
    "        xaxis=dict(showgrid=True,gridcolor='rgba(200,200,200,0.2)',linecolor='rgba(200,200,200,0.5)',linewidth=1,mirror=True,title='Generation'),\n",
    "        yaxis=dict(showgrid=True,gridcolor='rgba(200,200,200,0.2)',linecolor='rgba(200,200,200,0.5)',linewidth=1,mirror=True,title='Reward')\n",
    "    )\n",
    "\n",
    "    fig.update_traces(hovertemplate='%{y} en Generation %{x}<extra></extra>')\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6696b393",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "fill": "tozeroy",
         "fillcolor": "rgba(134, 210, 147, 0.2)",
         "hovertemplate": "%{y} en Generation %{x}<extra></extra>",
         "line": {
          "color": "#86D293",
          "width": 2.5
         },
         "mode": "lines",
         "name": "Best Reward",
         "type": "scatter",
         "x": {
          "bdata": "AQACAAMABAAFAAYABwAIAAkACgALAAwADQAOAA8AEAARABIAEwAUABUAFgAXABgAGQAaABsAHAAdAB4AHwAgACEAIgAjACQAJQAmACcAKAApACoAKwAsAC0ALgAvADAAMQAyADMANAA1ADYANwA4ADkAOgA7ADwAPQA+AD8AQABBAEIAQwBEAEUARgBHAEgASQBKAEsATABNAE4ATwBQAFEAUgBTAFQAVQBWAFcAWABZAFoAWwBcAF0AXgBfAGAAYQBiAGMAZABlAGYAZwBoAGkAagBrAGwAbQBuAG8AcABxAHIAcwB0AHUAdgB3AHgAeQB6AHsAfAB9AH4AfwCAAIEAggCDAIQAhQCGAIcAiACJAIoAiwCMAI0AjgCPAJAAkQCSAJMAlACVAJYAlwCYAJkAmgCbAJwAnQCeAJ8AoAChAKIAowCkAKUApgCnAKgAqQCqAKsArACtAK4ArwCwALEAsgCzALQAtQC2ALcAuAC5ALoAuwC8AL0AvgC/AMAAwQDCAMMAxADFAMYAxwDIAA==",
          "dtype": "i2"
         },
         "y": {
          "bdata": "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",
          "dtype": "f8"
         }
        },
        {
         "fill": "tozeroy",
         "fillcolor": "rgba(255, 207, 150, 0.2)",
         "hovertemplate": "%{y} en Generation %{x}<extra></extra>",
         "line": {
          "color": "#FFCF96",
          "width": 2.5
         },
         "mode": "lines",
         "name": "Mean Reward",
         "type": "scatter",
         "x": {
          "bdata": "AQACAAMABAAFAAYABwAIAAkACgALAAwADQAOAA8AEAARABIAEwAUABUAFgAXABgAGQAaABsAHAAdAB4AHwAgACEAIgAjACQAJQAmACcAKAApACoAKwAsAC0ALgAvADAAMQAyADMANAA1ADYANwA4ADkAOgA7ADwAPQA+AD8AQABBAEIAQwBEAEUARgBHAEgASQBKAEsATABNAE4ATwBQAFEAUgBTAFQAVQBWAFcAWABZAFoAWwBcAF0AXgBfAGAAYQBiAGMAZABlAGYAZwBoAGkAagBrAGwAbQBuAG8AcABxAHIAcwB0AHUAdgB3AHgAeQB6AHsAfAB9AH4AfwCAAIEAggCDAIQAhQCGAIcAiACJAIoAiwCMAI0AjgCPAJAAkQCSAJMAlACVAJYAlwCYAJkAmgCbAJwAnQCeAJ8AoAChAKIAowCkAKUApgCnAKgAqQCqAKsArACtAK4ArwCwALEAsgCzALQAtQC2ALcAuAC5ALoAuwC8AL0AvgC/AMAAwQDCAMMAxADFAMYAxwDIAA==",
          "dtype": "i2"
         },
         "y": {
          "bdata": "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",
          "dtype": "f8"
         }
        },
        {
         "fill": "tozeroy",
         "fillcolor": "rgba(255, 128, 128, 0.2)",
         "hovertemplate": "%{y} en Generation %{x}<extra></extra>",
         "line": {
          "color": "#FF8080",
          "width": 2.5
         },
         "mode": "lines",
         "name": "Worst Reward",
         "type": "scatter",
         "x": {
          "bdata": "AQACAAMABAAFAAYABwAIAAkACgALAAwADQAOAA8AEAARABIAEwAUABUAFgAXABgAGQAaABsAHAAdAB4AHwAgACEAIgAjACQAJQAmACcAKAApACoAKwAsAC0ALgAvADAAMQAyADMANAA1ADYANwA4ADkAOgA7ADwAPQA+AD8AQABBAEIAQwBEAEUARgBHAEgASQBKAEsATABNAE4ATwBQAFEAUgBTAFQAVQBWAFcAWABZAFoAWwBcAF0AXgBfAGAAYQBiAGMAZABlAGYAZwBoAGkAagBrAGwAbQBuAG8AcABxAHIAcwB0AHUAdgB3AHgAeQB6AHsAfAB9AH4AfwCAAIEAggCDAIQAhQCGAIcAiACJAIoAiwCMAI0AjgCPAJAAkQCSAJMAlACVAJYAlwCYAJkAmgCbAJwAnQCeAJ8AoAChAKIAowCkAKUApgCnAKgAqQCqAKsArACtAK4ArwCwALEAsgCzALQAtQC2ALcAuAC5ALoAuwC8AL0AvgC/AMAAwQDCAMMAxADFAMYAxwDIAA==",
          "dtype": "i2"
         },
         "y": {
          "bdata": "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",
          "dtype": "f8"
         }
        }
       ],
       "layout": {
        "font": {
         "color": "#333333",
         "family": "Helvetica, Arial, sans-serif",
         "size": 14
        },
        "height": 600,
        "legend": {
         "bgcolor": "rgba(255,255,255,0)",
         "bordercolor": "rgba(0,0,0,0)",
         "font": {
          "size": 14
         },
         "orientation": "h",
         "title": {
          "font": {
           "size": 16
          },
          "text": ""
         },
         "x": 0.5,
         "xanchor": "center",
         "y": 1.02,
         "yanchor": "bottom"
        },
        "paper_bgcolor": "white",
        "plot_bgcolor": "white",
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "font": {
          "color": "#333333",
          "size": 20
         },
         "text": "CMA-ES Training Results",
         "x": 0.5,
         "xanchor": "center",
         "y": 0.95,
         "yanchor": "top"
        },
        "width": 900,
        "xaxis": {
         "gridcolor": "rgba(200,200,200,0.2)",
         "linecolor": "rgba(200,200,200,0.5)",
         "linewidth": 1,
         "mirror": true,
         "showgrid": true,
         "title": {
          "text": "Generation"
         }
        },
        "yaxis": {
         "gridcolor": "rgba(200,200,200,0.2)",
         "linecolor": "rgba(200,200,200,0.5)",
         "linewidth": 1,
         "mirror": true,
         "showgrid": true,
         "title": {
          "text": "Reward"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "metrics = pd.read_csv('checkpoints/cma/cma_es_metrics.csv')\n",
    "plot_cma_es_results(metrics, colors=['#86D293', '#FFCF96', '#FF8080'], metric_columns=['best_reward', 'mean_reward', 'worst_reward'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21183d87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "controller = Controller(state_dim=LATENT_DIM + HIDDEN_DIM, action_dim=ACTION_DIM).to(device)\n",
    "controller.load_state_dict(torch.load('checkpoints/controller.pth', weights_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "268ef9fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward: 781.51 | Steps: 1000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "\n",
    "def render_policy(env_name, controller, mdnrnn, encode_obs_batch):\n",
    "    \"\"\"\n",
    "    Render a policy in the environment.\n",
    "    \n",
    "    Args:\n",
    "        env_name (str): Name of the environment\n",
    "        controller: Trained controller model\n",
    "        mdnrnn: LSTM-MDN memory model\n",
    "        encode_obs_batch: Function to encode observations\n",
    "    \"\"\"\n",
    "    # Create environment with human rendering\n",
    "    env = gym.make(env_name, render_mode='human', lap_complete_percent=1.0)\n",
    "\n",
    "    done = False\n",
    "    cumulative_reward = 0\n",
    "    obs, _ = env.reset()\n",
    "    \n",
    "    # Initialize hidden state\n",
    "    h = (torch.zeros(1, HIDDEN_DIM).to(device),\n",
    "         torch.zeros(1, HIDDEN_DIM).to(device))\n",
    "    step_count = 1\n",
    "    \n",
    "    while True:\n",
    "        # Encode observation to latent space\n",
    "        z = encode_obs_batch(obs[np.newaxis, ...])\n",
    "\n",
    "        # Combine latent and hidden state\n",
    "        x = torch.cat([z, h[0]], dim=-1)\n",
    "        \n",
    "        # Get action from controller\n",
    "        a = controller.get_action(x)\n",
    "           \n",
    "        # Step environment\n",
    "        obs, reward, done, _, _ = env.step(a.detach().cpu().numpy())\n",
    "        env.render()\n",
    "        \n",
    "        # Update LSTM hidden state\n",
    "        _, h = mdnrnn.rnn(z, a.unsqueeze(0), h=h)\n",
    "    \n",
    "        cumulative_reward += reward\n",
    "        step_count += 1\n",
    "        \n",
    "        # End episode on completion or timeout\n",
    "        if done or step_count >= 1000:\n",
    "            break\n",
    "    \n",
    "    env.close()\n",
    "    print(f'Reward: {cumulative_reward:.2f} | Steps: {step_count}')\n",
    "    \n",
    "# Visualize the trained controller in action\n",
    "render_policy('CarRacing-v3', controller, memory, encode_obs_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a02695",
   "metadata": {},
   "source": [
    "## **Final Evaluation**\n",
    "---\n",
    "\n",
    "To rigorously evaluate the performance of our trained controller, we run it for 100 episodes and calculate the mean reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9034a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import numpy as np\n",
    "from torch.nn.utils import parameters_to_vector\n",
    "from tqdm import tqdm\n",
    "\n",
    "def final_evaluation(controller_class, best_solution, memory, \n",
    "                     parallel_rollouts=7, max_steps=1000, popsize=16):\n",
    "    \"\"\"\n",
    "    Comprehensive evaluation of the best controller across multiple episodes.\n",
    "    \n",
    "    Args:\n",
    "        controller_class: Controller class to evaluate\n",
    "        best_solution: Parameter vector of the best controller\n",
    "        memory: LSTM-MDN memory model\n",
    "        parallel_rollouts: Number of parallel evaluation batches\n",
    "        max_steps: Maximum steps per episode\n",
    "        popsize: Number of parallel environments\n",
    "        \n",
    "    Returns:\n",
    "        list: All rewards from evaluation\n",
    "    \"\"\"\n",
    "    final_rewards = []\n",
    "    best_controllers = [best_solution for _ in range(popsize)] \n",
    "    \n",
    "    # Execute multiple evaluation rollouts\n",
    "    for i in tqdm(range(parallel_rollouts)):\n",
    "        rewards = evaluate_policies(best_controllers, controller_class, \n",
    "                                  max_steps, memory)\n",
    "        final_rewards.append(rewards)\n",
    "\n",
    "    # Flatten rewards list and calculate statistics\n",
    "    all_rewards = np.array(final_rewards).flatten()\n",
    "    print(f\"Performance over {popsize*parallel_rollouts} episodes: \"\n",
    "          f\"{np.mean(all_rewards):.2f}  {np.std(all_rewards):.2f}\")\n",
    "    return all_rewards\n",
    "\n",
    "# Load the best controller\n",
    "controller = Controller(LATENT_DIM+HIDDEN_DIM, ACTION_DIM).to(device)\n",
    "controller.load_state_dict(torch.load('checkpoints/controller.pth', weights_only=True))\n",
    "\n",
    "# Extract controller parameters\n",
    "controller_best_solution = parameters_to_vector(\n",
    "    controller.parameters()).cpu().detach().numpy()\n",
    "\n",
    "# Evaluation parameters\n",
    "popsize = 16\n",
    "parallel_rollouts = 8\n",
    "\n",
    "# Run final evaluation\n",
    "rewards = final_evaluation(Controller, controller_best_solution, memory, \n",
    "                         parallel_rollouts=parallel_rollouts, \n",
    "                         max_steps=1000, popsize=popsize)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
